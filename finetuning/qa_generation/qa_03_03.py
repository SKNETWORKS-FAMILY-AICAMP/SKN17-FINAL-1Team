import json
import random
import base64
from pathlib import Path
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()
client = OpenAI()


MALE_CUTS = [
    "ê°€ì¼ì»·", "ëŒ„ë””ì»·", "ë“œëì»·", "ë¦¬ì  íŠ¸ì»·", "ë¦¬í”„ì»·", "ë²„ì¦ˆì»·", "ìŠ¬ë¦­ë°±ì»·",
    "ì•„ì´ë¸Œë¦¬ê·¸ì»·", "ìš¸í”„ì»·", "í¬ë¡­ì»·", "í¬ë£¨ì»·", "íˆ¬ë¸”ëŸ­ì»·", "í˜ì´ë“œì»·",
    "í¬ë§ˆë“œì»·", "í•„ëŸ¬ìŠ¤ì»·", "í•˜ì´ì•¤íƒ€ì´íŠ¸ì»·", "ê°€ë¥´ë§ˆíŒ"
]

MALE_PERMS = [
    "ê°€ì¼íŒ", "ë‚´ì¶”ëŸ´íŒ", "ëŒ„ë””íŒ", "ë¦¬ì  íŠ¸íŒ", "ë¦¬í”„íŒ", "ë² ì´ë¹„íŒ", "ë³¼ë¥¨íŒ",
    "ì‰ë„ìš°íŒ", "ìŠ¤ì™ˆë¡œíŒ", "ì• ì¦ˆíŒ", "ì›¨ì´ë¸ŒíŒ", "í¬ë¦¬ë“œíŒ", "í¬ë§ˆë“œíŒ", "íˆí”¼íŒ"
]

FEMALE_CUTS = [
    "ë ˆì´ì–´ë“œì»·", "ë¦¬í”„ì»·", "ë¨¸ì‰¬ë£¸ì»·", "ë±…í—¤ì–´", "ë³´ë¸Œì»·", "ìƒ¤ê¸°ì»·",
    "ì›ë­ìŠ¤ì»·", "í”½ì‹œì»·", "í—ˆì‰¬ì»·", "íˆë©”ì»·"
]

FEMALE_PERMS = [
    "CSì»¬íŒ", "Cì»¬íŒ", "Sì»¬íŒ", "ê¸€ë¨íŒ", "ë‚´ì¸„ëŸ´íŒ", "ë””ì§€í„¸íŒ", "ëŸ¬ë¸”ë¦¬íŒ",
    "ë ˆì´ì–´ë“œíŒ", "ë£¨ì¦ˆíŒ", "ë¦¬í”„íŒ", "ë¬¼ê²°íŒ", "ë¯¹ìŠ¤íŒ", "ë°”ë””íŒ", "ë°œë¡±íŒ",
    "ë³¼ë“œíŒ", "ë³¼ë¥¨ë§¤ì§", "ë³¼ë¥¨íŒ", "ë¹Œë“œíŒ", "ì…‹íŒ…íŒ", "ìŠ¤íŒŒì´ëŸ´íŒ", "ì—ì–´íŒ",
    "ì ¤ë¦¬íŒ", "ì§€ì ¤íŒ", "ì¿ ì…˜íŒ", "í…ìŠ¤ì²˜íŒ", "í¼í”¼ë² ì´ë¹„íŒ", "í—ˆì‰¬íŒ", "íˆí”¼íŒ"
]

HAIR_COLORS = [
    "ê³¨ë“œë¸Œë¼ìš´", "ë‹¤í¬ë¸Œë¼ìš´", "ë ˆë“œë¸Œë¼ìš´", "ë ˆë“œì™€ì¸", "ë¡œì¦ˆê³¨ë“œ", "ë§ˆë¥´ì‚´ë¼",
    "ë§ˆí˜¸ê°€ë‹ˆ", "ë°€í¬ë¸Œë¼ìš´", "ë² ì´ì§€ë¸Œë¼ìš´", "ë¸”ë£¨ë¸”ë™", "ì• ì‰¬ê·¸ë ˆì´", "ì• ì‰¬ë°”ì´ì˜¬ë ›",
    "ì• ì‰¬ë² ì´ì§€", "ì• ì‰¬ë¸Œë¼ìš´", "ì• ì‰¬ë¸”ë¡ ë“œ", "ì• ì‰¬ë¸”ë£¨", "ì• ì‰¬ì¹´í‚¤", "ì• ì‰¬í¼í”Œ",
    "ì˜¤ë Œì§€ë¸Œë¼ìš´", "ì˜¬ë¦¬ë¸Œë¸Œë¼ìš´", "ì´ˆì½”ë¸Œë¼ìš´", "ì¹´í‚¤ë¸Œë¼ìš´", "ì¿ í¼ë¸Œë¼ìš´", "í•‘í¬ë¸Œë¼ìš´"
]

MALE_LENGTHS = ["ìˆ", "ë¯¸ë””ì—„", "ì¥ë°œ"]
FEMALE_LENGTHS = ["ìˆ", "ë‹¨ë°œ", "ì¤‘ë‹¨ë°œ", "ë¯¸ë””ì—„", "ì¥ë°œ"]

LENGTH_EXPRESSIONS = {
    "male": {
        "ìˆ": ["ì§§ê²Œ", "ìˆìœ¼ë¡œ", "ì§§ì€ ë¨¸ë¦¬ë¡œ", "ê·€ ìœ„ë¡œ", "ëª© ë“œëŸ¬ë‚˜ê²Œ", "ì‹œì›í•˜ê²Œ ì§§ê²Œ", "ìˆì»·ìœ¼ë¡œ"],
        "ë¯¸ë””ì—„": ["ì¤‘ê°„ ê¸¸ì´ë¡œ", "ë¯¸ë””ì—„ìœ¼ë¡œ", "ê·€ ì•„ë˜ ì •ë„ë¡œ", "í„±ì„  ê¸¸ì´ë¡œ", "ê°€ë¥´ë§ˆ ìŠ¤íƒ€ì¼ë¡œ"],
        "ì¥ë°œ": ["ê¸¸ê²Œ", "ì¥ë°œë¡œ", "ì–´ê¹¨ê¹Œì§€", "ê¸´ë¨¸ë¦¬ë¡œ", "ì›¨ì´ë¸Œ ë„£ì„ ìˆ˜ ìˆê²Œ ê¸¸ê²Œ"]
    },
    "female": {
        "ìˆ": ["ì§§ê²Œ", "ìˆìœ¼ë¡œ", "ê·€ ì•„ë˜ë¡œ", "ìˆì»·ìœ¼ë¡œ", "ì§§ì€ ë¨¸ë¦¬ë¡œ"],
        "ë‹¨ë°œ": ["ë‹¨ë°œë¡œ", "í„±ì„  ê¸¸ì´ë¡œ", "í„± ì•„ë˜ë¡œ", "ë‹¨ë°œë¨¸ë¦¬ë¡œ"],
        "ì¤‘ë‹¨ë°œ": ["ì¤‘ë‹¨ë°œë¡œ", "ì–´ê¹¨ ë‹¿ëŠ” ê¸¸ì´ë¡œ", "ì–´ê¹¨ì„ ìœ¼ë¡œ", "ì–´ê¹¨ ìœ„ë¡œ"],
        "ë¯¸ë””ì—„": ["ë¯¸ë””ì—„ìœ¼ë¡œ", "ì‡„ê³¨ ê¸¸ì´ë¡œ", "ì„¸ë¯¸ë¡±ìœ¼ë¡œ", "ì–´ê¹¨ ì•„ë˜ë¡œ"],
        "ì¥ë°œ": ["ê¸¸ê²Œ", "ì¥ë°œë¡œ", "ê°€ìŠ´ê¹Œì§€", "ê¸´ë¨¸ë¦¬ë¡œ", "ë¡±í—¤ì–´ë¡œ", "ì‡„ê³¨ ì•„ë˜ë¡œ"]
    }
}


def generate_exception_queries(num_samples=10):
    """
    2ê°€ì§€ ì´ë¯¸ì§€ ì˜ˆì™¸ ì¼€ì´ìŠ¤ ìƒì„±:
      - no_face: ì–¼êµ´ ì—†ëŠ” ì´ë¯¸ì§€ (í’ê²½, ì‚¬ë¬¼, ìŒì‹ ë“±)
      - multi_face: 2ëª… ì´ìƒ ë‚˜ì˜¨ ì´ë¯¸ì§€

    ì¤‘ìš”: ì‚¬ìš©ìëŠ” ì •ìƒì ì¸ í—¤ì–´ìŠ¤íƒ€ì¼ ë³€í™˜ ìš”ì²­ì„ í•˜ì§€ë§Œ,
          ì´ë¯¸ì§€ì— ë¬¸ì œê°€ ìˆì–´ì„œ ì˜ˆì™¸ ì‘ë‹µì´ ë‚˜ì™€ì•¼ í•˜ëŠ” ìƒí™©
    """

    prompt = f"""
í—¤ì–´ìŠ¤íƒ€ì¼ ì´ë¯¸ì§€ ìƒì„± ì±—ë´‡ì˜ ì˜ˆì™¸ì²˜ë¦¬ í•™ìŠµ ë°ì´í„°ë¥¼ ë§Œë“­ë‹ˆë‹¤.

[ì‹œë‚˜ë¦¬ì˜¤]
ì‚¬ìš©ìê°€ ìì‹ ì˜ ì‚¬ì§„ì„ ì—…ë¡œë“œí•˜ê³  ì •ìƒì ìœ¼ë¡œ í—¤ì–´ìŠ¤íƒ€ì¼ ë³€í™˜ì„ ìš”ì²­í•©ë‹ˆë‹¤.
í•˜ì§€ë§Œ ì—…ë¡œë“œëœ ì´ë¯¸ì§€ì— ë¬¸ì œê°€ ìˆì–´ì„œ ëª¨ë¸ì´ ì˜ˆì™¸ ì‘ë‹µì„ í•´ì•¼ í•˜ëŠ” ìƒí™©ì…ë‹ˆë‹¤.

[ì˜ˆì™¸ ì¼€ì´ìŠ¤ 2ê°€ì§€]
1. no_face: ì´ë¯¸ì§€ì— ì–¼êµ´ì´ ì—†ìŒ (í’ê²½, ìŒì‹, ì‚¬ë¬¼, ë™ë¬¼ ì‚¬ì§„ ë“±)
   â†’ ì‘ë‹µ: "ì–¼êµ´ì´ í¬í•¨ëœ ì´ë¯¸ì§€ë¥¼ ì²¨ë¶€í•˜ì…”ì•¼ ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤ğŸ¥² í™•ì¸ í›„ ë‹¤ë¥¸ ì‚¬ì§„ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."

2. multi_face: ì´ë¯¸ì§€ì— 2ëª… ì´ìƒì˜ ì–¼êµ´ì´ ìˆìŒ (ë‹¨ì²´ì‚¬ì§„, ì»¤í”Œì‚¬ì§„ ë“±)
   â†’ ì‘ë‹µ: "ì´ ì´ë¯¸ì§€ì—ëŠ” 2ëª… ì´ìƒì˜ ì–¼êµ´ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤ğŸ¥² í•œ ëª…ë§Œ ë‚˜ì˜¨ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."

[ì¤‘ìš”: ì‚¬ìš©ì ì§ˆì˜ íŠ¹ì§•]
- ì‚¬ìš©ìëŠ” ì´ë¯¸ì§€ì— ë¬¸ì œê°€ ìˆë‹¤ëŠ” ê±¸ ëª¨ë¦…ë‹ˆë‹¤
- ì •ìƒì ì¸ í—¤ì–´ìŠ¤íƒ€ì¼ ë³€í™˜ ìš”ì²­ì„ í•©ë‹ˆë‹¤
- "íˆë©”ì»·ìœ¼ë¡œ ë°”ê¿”ì¤˜", "ë ˆë“œì™€ì¸ ì»¬ëŸ¬ë¡œ ì—¼ìƒ‰í•´ì¤˜", "ë‹¨ë°œë¡œ ì˜ë¼ì¤˜" ê°™ì€ ì¼ë°˜ì ì¸ ìš”ì²­
- ì ˆëŒ€ë¡œ "ì–¼êµ´ì´ ì—†ëŠ”ë°", "ì‚¬ì§„ì— ë‘ëª…ì´ ë‚˜ì™”ëŠ”ë°" ê°™ì€ í‘œí˜„ ì‚¬ìš© ê¸ˆì§€

[ì§ˆì˜ ì˜ˆì‹œ]
- "ì´ ì‚¬ì§„ìœ¼ë¡œ íˆë©”ì»·ì— ë ˆë“œì™€ì¸ ì»¬ëŸ¬ ì ìš©í•´ì¤˜"
- "í—ˆì‰¬íŒìœ¼ë¡œ ë°”ê¿”ì£¼ì„¸ìš”"
- "ì• ì‰¬ë¸Œë¼ìš´ìœ¼ë¡œ ì—¼ìƒ‰í•˜ê³  ì‹¶ì–´ìš”"
- "ë‹¨ë°œë¡œ ì˜ë¼ì£¼ê³  ë³¼ë¥¨íŒ ë„£ì–´ì¤˜"
- "ì´ ì–¼êµ´ì— ìš¸í”„ì»· ì–´ìš¸ë¦´ê¹Œ? ì ìš©í•´ë´ì¤˜"

[ìƒì„± ê·œì¹™]
- ì´ {num_samples}ê°œ ìƒì„± (no_face, multi_face ê· ë“± ë¶„ë°°)
- ë°˜ë§/ì¡´ëŒ“ë§ ì„ê¸°, ì´ëª¨ì§€ ê°€ë” ì‚¬ìš©
- í‘œí˜„ ë‹¤ì–‘í™”: "ì´ ì‚¬ì§„ìœ¼ë¡œ", "ë‚´ ì–¼êµ´ì—", "ì´ ì´ë¯¸ì§€ë¡œ", "ë°”ê¿”ì¤˜", "ì ìš©í•´ì¤˜", "ë³€í™˜í•´ì¤˜" ë“±

[ì¶œë ¥: JSON ë°°ì—´ë§Œ]
{{
  "type": "no_face" | "multi_face",
  "user": "ì‚¬ìš©ìì˜ ì •ìƒì ì¸ í—¤ì–´ìŠ¤íƒ€ì¼ ë³€í™˜ ìš”ì²­"
}}

JSON ë°°ì—´ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.9,
    )

    content = response.choices[0].message.content.strip()

    if content.startswith("```"):
        lines = content.split("\n")
        content = "\n".join(lines[1:-1]) if len(lines) > 2 else content
        content = content.strip()

    if content.startswith("json"):
        content = content[4:].strip()

    try:
        return json.loads(content)
    except json.JSONDecodeError as e:
        print(f"[ê²½ê³ ] JSON íŒŒì‹± ì—ëŸ¬ (generate_exception_queries): {e}")
        print(f"[ë””ë²„ê·¸] ì²« 100ì: {content[:100]}")

        bracket_count = 0
        end_idx = -1
        for i, char in enumerate(content):
            if char == '[':
                bracket_count += 1
            elif char == ']':
                bracket_count -= 1
                if bracket_count == 0:
                    end_idx = i + 1
                    break

        if end_idx > 0:
            valid_content = content[:end_idx]
            print(f"[ë””ë²„ê·¸] ìœ íš¨í•œ ë¶€ë¶„ë§Œ ì¶”ì¶œ: {len(valid_content)}ì")
            return json.loads(valid_content)
        else:
            raise


def generate_normal_queries_with_length(num_samples=10):
    """
    2ê°€ì§€ ìŠ¤íƒ€ì¼/ì»¬ëŸ¬ ì˜ˆì™¸ ì¼€ì´ìŠ¤ ìƒì„±:
      - unsupported_style: ì§€ì›í•˜ì§€ ì•ŠëŠ” í—¤ì–´ìŠ¤íƒ€ì¼/ì»¬ëŸ¬ ì´ë¦„ ìš”ì²­
      - missing_style: ìŠ¤íƒ€ì¼/ì»¬ëŸ¬ë¥¼ ì•„ì˜ˆ ì§€ì •í•˜ì§€ ì•ŠìŒ

    ì¤‘ìš”: ì‚¬ìš©ìê°€ ì´ë¯¸ì§€ ë³€í™˜ì„ ìš”ì²­í•˜ì§€ë§Œ,
          ìŠ¤íƒ€ì¼ëª…ì´ ì§€ì› ëª©ë¡ì— ì—†ê±°ë‚˜ ì•„ë¬´ê²ƒë„ ì•ˆ ì ì–´ì„œ ì˜ˆì™¸ ì‘ë‹µ í•„ìš”
    """

    male_styles = ", ".join(MALE_CUTS + MALE_PERMS)
    female_styles = ", ".join(FEMALE_CUTS + FEMALE_PERMS)
    colors = ", ".join(HAIR_COLORS)

    prompt = f"""
í—¤ì–´ìŠ¤íƒ€ì¼ ì´ë¯¸ì§€ ìƒì„± ì±—ë´‡ì˜ ìŠ¤íƒ€ì¼/ì»¬ëŸ¬ ì˜ˆì™¸ì²˜ë¦¬ í•™ìŠµ ë°ì´í„°ë¥¼ ë§Œë“­ë‹ˆë‹¤.

[ì‹œë‚˜ë¦¬ì˜¤]
ì‚¬ìš©ìê°€ ìì‹ ì˜ ì–¼êµ´ ì‚¬ì§„(ì •ìƒ)ì„ ì—…ë¡œë“œí•˜ê³  í—¤ì–´ìŠ¤íƒ€ì¼ ë³€í™˜ì„ ìš”ì²­í•©ë‹ˆë‹¤.
í•˜ì§€ë§Œ ìŠ¤íƒ€ì¼ëª…ì´ ì§€ì› ëª©ë¡ì— ì—†ê±°ë‚˜ ì•„ë¬´ê²ƒë„ ì§€ì •í•˜ì§€ ì•Šì•„ì„œ ì˜ˆì™¸ ì‘ë‹µì´ í•„ìš”í•œ ìƒí™©ì…ë‹ˆë‹¤.

[ì˜ˆì™¸ ì¼€ì´ìŠ¤ 2ê°€ì§€]
1. unsupported_style: ì§€ì›í•˜ì§€ ì•ŠëŠ” í—¤ì–´ìŠ¤íƒ€ì¼/ì»¬ëŸ¬ ì´ë¦„ ìš”ì²­
   - ì‚¬ìš©ìê°€ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ì§€ì› ì•ˆë˜ëŠ” ìŠ¤íƒ€ì¼ëª… ìš”ì²­
   - ë˜ëŠ” "ì²­ìˆœí•˜ê²Œ", "ì‹œí¬í•˜ê²Œ", "ë©‹ìˆê²Œ" ê°™ì´ êµ¬ì²´ì ì¸ ìŠ¤íƒ€ì¼ëª…ì´ ì•„ë‹Œ í˜•ìš©ì‚¬/ì¶”ìƒì  í‘œí˜„
   â†’ ì‘ë‹µ: "ìš”ì²­í•˜ì‹  ìŠ¤íƒ€ì¼ì€ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ğŸ¥²\\n\\nì§€ì› ê°€ëŠ¥í•œ ìŠ¤íƒ€ì¼/ì»¬ëŸ¬ ëª©ë¡ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ìš”ì²­í•´ì£¼ì„¸ìš”!"

2. missing_style: ìŠ¤íƒ€ì¼/ì»¬ëŸ¬ë¥¼ ì•„ì˜ˆ ì§€ì •í•˜ì§€ ì•ŠìŒ
   - "ì´ ì‚¬ì§„ìœ¼ë¡œ ë¨¸ë¦¬ ë°”ê¿”ì¤˜", "í—¤ì–´ìŠ¤íƒ€ì¼ ë°”ê¿”ì£¼ì„¸ìš”", "ë³€ì‹ ì‹œì¼œì¤˜" ê°™ì´ êµ¬ì²´ì ì¸ ìŠ¤íƒ€ì¼/ì»¬ëŸ¬ ì—†ìŒ
   â†’ ì‘ë‹µ: "êµ¬ì²´ì ì¸ í—¤ì–´ìŠ¤íƒ€ì¼ì´ë‚˜ ì»¬ëŸ¬ë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”!"

[ì§€ì› ê°€ëŠ¥í•œ ëª©ë¡]
- ë‚¨ì ìŠ¤íƒ€ì¼: {male_styles}
- ì—¬ì ìŠ¤íƒ€ì¼: {female_styles}
- ì»¬ëŸ¬: {colors}

[ì§ˆì˜ ì˜ˆì‹œ]
unsupported_style (ì§€ì› ì•ˆë˜ëŠ” ì´ë¦„):
- "íƒœìŠ¬íŒìœ¼ë¡œ ë°”ê¿”ì¤˜" (ì¡´ì¬í•˜ì§€ ì•ŠìŒ)
- "ê·¸ë¦°ì»¬ëŸ¬ë¡œ ì—¼ìƒ‰í•´ì¤˜" (ì§€ì› ì•ˆí•¨)
- "ì²­ìˆœí•œ ëŠë‚Œìœ¼ë¡œ ë°”ê¿”ì¤˜" (ì¶”ìƒì  í‘œí˜„)
- "ê³ ëŒ€í’ í—¤ì–´ìŠ¤íƒ€ì¼ë¡œ" (ì§€ì› ì•ˆí•¨)
- "ë„¤ì˜¨ í•‘í¬ë¡œ ì—¼ìƒ‰í•´ì£¼ì„¸ìš”" (ì§€ì› ì•ˆí•¨)
- "ì‹œí¬í•˜ê³  ë©‹ìˆê²Œ í•´ì¤˜" (í˜•ìš©ì‚¬ë§Œ)
- "ì—˜ì‚¬ ë¨¸ë¦¬ë¡œ" (ìºë¦­í„° ì´ë¦„)
- "ë²„ì„¯ ì»·ìœ¼ë¡œ" (ì˜¤íƒ€/ë‹¤ë¥¸ ì´ë¦„)

missing_style (ìŠ¤íƒ€ì¼/ì»¬ëŸ¬ ì—†ìŒ):
- "ì´ ì‚¬ì§„ìœ¼ë¡œ ë¨¸ë¦¬ ë°”ê¿”ì¤˜"
- "í—¤ì–´ìŠ¤íƒ€ì¼ ë³€ì‹ ì‹œì¼œì£¼ì„¸ìš”"
- "ì´ë¯¸ì§€ ë°”ê¿”ì¤„ë˜?"
- "ë¨¸ë¦¬ ì¢€ ë°”ê¾¸ê³  ì‹¶ì–´"
- "ë‹¤ë¥¸ ìŠ¤íƒ€ì¼ë¡œ í•´ì¤˜"
- "ì–¼êµ´ ì¢€ ë°”ê¿”ë´ì¤˜"
- "ë³€ì‹  ë¶€íƒí•´"

[ìƒì„± ê·œì¹™]
- ì´ {num_samples}ê°œ ìƒì„± (unsupported_style, missing_style ê· ë“± ë¶„ë°°)
- ë°˜ë§/ì¡´ëŒ“ë§ ì„ê¸°, ì´ëª¨ì§€ ê°€ë” ì‚¬ìš©
- unsupported_style: ì‹¤ì œë¡œ ì—†ê±°ë‚˜ ì§€ì› ì•ˆë˜ëŠ” ì´ë¦„, í˜•ìš©ì‚¬, ì¶”ìƒì  í‘œí˜„ ì‚¬ìš© (ì°½ì˜ì ìœ¼ë¡œ!)
- missing_style: êµ¬ì²´ì  ìŠ¤íƒ€ì¼ëª… ì ˆëŒ€ í¬í•¨ ê¸ˆì§€, "ë¨¸ë¦¬", "ìŠ¤íƒ€ì¼", "í—¤ì–´" ê°™ì€ ì¼ë°˜ ëª…ì‚¬ë§Œ

[ì¶œë ¥: JSON ë°°ì—´ë§Œ]
{{
  "type": "unsupported_style" | "missing_style",
  "user": "ì‚¬ìš©ì ì§ˆì˜"
}}

JSON ë°°ì—´ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.9,
    )

    content = response.choices[0].message.content.strip()

    if content.startswith("```"):
        lines = content.split("\n")
        content = "\n".join(lines[1:-1]) if len(lines) > 2 else content
        content = content.strip()

    if content.startswith("json"):
        content = content[4:].strip()

    try:
        return json.loads(content)
    except json.JSONDecodeError as e:
        print(f"[ê²½ê³ ] JSON íŒŒì‹± ì—ëŸ¬ (generate_normal_queries_with_length): {e}")
        print(f"[ë””ë²„ê·¸] ì²« 100ì: {content[:100]}")

        bracket_count = 0
        end_idx = -1
        for i, char in enumerate(content):
            if char == '[':
                bracket_count += 1
            elif char == ']':
                bracket_count -= 1
                if bracket_count == 0:
                    end_idx = i + 1
                    break

        if end_idx > 0:
            valid_content = content[:end_idx]
            print(f"[ë””ë²„ê·¸] ìœ íš¨í•œ ë¶€ë¶„ë§Œ ì¶”ì¶œ: {len(valid_content)}ì")
            return json.loads(valid_content)
        else:
            raise


RESPONSE_MAP = {
    "no_face":
        "ì–¼êµ´ì´ í¬í•¨ëœ ì´ë¯¸ì§€ë¥¼ ì²¨ë¶€í•˜ì…”ì•¼ ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤ğŸ¥² í™•ì¸ í›„ ë‹¤ë¥¸ ì‚¬ì§„ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.",

    "multi_face":
        "ì´ ì´ë¯¸ì§€ì—ëŠ” 2ëª… ì´ìƒì˜ ì–¼êµ´ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤ğŸ¥² í•œ ëª…ë§Œ ë‚˜ì˜¨ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.",

    "unsupported_style":
        "ì£„ì†¡í•©ë‹ˆë‹¤ğŸ¥² ìš”ì²­í•˜ì‹  í—¤ì–´ìŠ¤íƒ€ì¼/ì»¬ëŸ¬ëŠ” í˜„ì¬ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì•„ë˜ ì˜µì…˜ ëª©ë¡ì—ì„œ ì„ íƒí•´ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\n\n"
        "**ì§€ì› ìŠ¤íƒ€ì¼**\n"
        "ë‚¨ì ì»·: ê°€ì¼ì»·, ëŒ„ë””ì»·, ë“œëì»·, ë¦¬ì  íŠ¸ì»·, ë¦¬í”„ì»·, ë²„ì¦ˆì»·, ìŠ¬ë¦­ë°±ì»·, ì•„ì´ë¸Œë¦¬ê·¸ì»·, ìš¸í”„ì»·, í¬ë¡­ì»·, í¬ë£¨ì»·, íˆ¬ë¸”ëŸ­ì»·, í˜ì´ë“œì»·, í¬ë§ˆë“œì»·, í•„ëŸ¬ìŠ¤ì»·, í•˜ì´ì•¤íƒ€ì´íŠ¸ì»·, ê°€ë¥´ë§ˆíŒ\n"
        "ë‚¨ì íŒ: ê°€ì¼íŒ, ë‚´ì¶”ëŸ´íŒ, ëŒ„ë””íŒ, ë¦¬ì  íŠ¸íŒ, ë¦¬í”„íŒ, ë² ì´ë¹„íŒ, ë³¼ë¥¨íŒ, ì‰ë„ìš°íŒ, ìŠ¤ì™ˆë¡œíŒ, ì• ì¦ˆíŒ, ì›¨ì´ë¸ŒíŒ, í¬ë¦¬ë“œíŒ, í¬ë§ˆë“œíŒ, íˆí”¼íŒ\n"
        "ì—¬ì ì»·: ë ˆì´ì–´ë“œì»·, ë¦¬í”„ì»·, ë¨¸ì‰¬ë£¸ì»·, ë±…í—¤ì–´, ë³´ë¸Œì»·, ìƒ¤ê¸°ì»·, ì›ë­ìŠ¤ì»·, í”½ì‹œì»·, í—ˆì‰¬ì»·, íˆë©”ì»·\n"
        "ì—¬ì íŒ: CSì»¬íŒ, Cì»¬íŒ, Sì»¬íŒ, ê¸€ë¨íŒ, ë‚´ì¸„ëŸ´íŒ, ë””ì§€í„¸íŒ, ëŸ¬ë¸”ë¦¬íŒ, ë ˆì´ì–´ë“œíŒ, ë£¨ì¦ˆíŒ, ë¦¬í”„íŒ, ë¬¼ê²°íŒ, ë¯¹ìŠ¤íŒ, ë°”ë””íŒ, ë°œë¡±íŒ, ë³¼ë“œíŒ, ë³¼ë¥¨ë§¤ì§, ë³¼ë¥¨íŒ, ë¹Œë“œíŒ, ì…‹íŒ…íŒ, ìŠ¤íŒŒì´ëŸ´íŒ, ì—ì–´íŒ, ì ¤ë¦¬íŒ, ì§€ì ¤íŒ, ì¿ ì…˜íŒ, í…ìŠ¤ì²˜íŒ, í¼í”¼ë² ì´ë¹„íŒ, í—ˆì‰¬íŒ, íˆí”¼íŒ\n\n"
        "**ì§€ì› ì»¬ëŸ¬**\n"
        "ê³¨ë“œë¸Œë¼ìš´, ë‹¤í¬ë¸Œë¼ìš´, ë ˆë“œë¸Œë¼ìš´, ë ˆë“œì™€ì¸, ë¡œì¦ˆê³¨ë“œ, ë§ˆë¥´ì‚´ë¼, ë§ˆí˜¸ê°€ë‹ˆ, ë°€í¬ë¸Œë¼ìš´, ë² ì´ì§€ë¸Œë¼ìš´, ë¸”ë£¨ë¸”ë™, ì• ì‰¬ê·¸ë ˆì´, ì• ì‰¬ë°”ì´ì˜¬ë ›, ì• ì‰¬ë² ì´ì§€, ì• ì‰¬ë¸Œë¼ìš´, ì• ì‰¬ë¸”ë¡ ë“œ, ì• ì‰¬ë¸”ë£¨, ì• ì‰¬ì¹´í‚¤, ì• ì‰¬í¼í”Œ, ì˜¤ë Œì§€ë¸Œë¼ìš´, ì˜¬ë¦¬ë¸Œë¸Œë¼ìš´, ì´ˆì½”ë¸Œë¼ìš´, ì¹´í‚¤ë¸Œë¼ìš´, ì¿ í¼ë¸Œë¼ìš´, í•‘í¬ë¸Œë¼ìš´",

    "missing_style":
        "ì–´ë–¤ í—¤ì–´ìŠ¤íƒ€ì¼ì´ë‚˜ í—¤ì–´ì»¬ëŸ¬ë¡œ ë³€ê²½í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”? ì›í•˜ì‹œëŠ” ìŠ¤íƒ€ì¼ì´ë‚˜ ì»¬ëŸ¬ë¥¼ ë§ì”€í•´ì£¼ì„¸ìš”ğŸ˜Š"
}


def build_tool_call(sample):
    """ì •ìƒ ì¼€ì´ìŠ¤ì— ëŒ€í•œ tool_call í˜•ì‹ ìƒì„±"""
    params = {}

    if sample.get("hairstyle"):
        params["hairstyle"] = sample["hairstyle"]
    if sample.get("haircolor"):
        params["haircolor"] = sample["haircolor"]
    if sample.get("hairlength"):
        params["hairlength"] = sample["hairlength"]

    return {
        "name": "hairstyle_generation_tool",
        "parameters": params
    }


def convert_exception_to_training_format(samples):
    """ì˜ˆì™¸ ì¼€ì´ìŠ¤ â†’ í•™ìŠµ í¬ë§· (ì´ë¯¸ì§€ëŠ” build_dataset.pyì—ì„œ ë§¤ì¹­)"""
    training_data = []

    for s in samples:
        stype = s["type"]
        assistant_reply = RESPONSE_MAP[stype]

        if stype == "no_face":
            image_type = "no_face"
        elif stype == "multi_face":
            image_type = "multi_face"
        else:
            image_type = "normal"

        training_data.append({
            "messages": [
                {"role": "system", "content": ""},
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": s["user"]},
                        {"type": "image_url", "image_url": {"url": ""}}
                    ]
                },
                {"role": "assistant", "content": assistant_reply}
            ],
            "image_type": image_type
        })

    return training_data


def convert_normal_to_training_format(samples):
    """
    ì •ìƒ(ì˜ˆì™¸) ì¼€ì´ìŠ¤ â†’ í•™ìŠµ í¬ë§·
    unsupported_style, missing_styleì€ í…ìŠ¤íŠ¸ ì‘ë‹µë§Œ
    ì´ë¯¸ì§€ëŠ” build_dataset.pyì—ì„œ ë§¤ì¹­
    """
    training_data = []

    for i, s in enumerate(samples):
        stype = s["type"]
        assistant_reply = RESPONSE_MAP[stype]

        training_data.append({
            "messages": [
                {"role": "system", "content": ""},
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": s["user"]},
                        {"type": "image_url", "image_url": {"url": ""}}
                    ]
                },
                {"role": "assistant", "content": assistant_reply}
            ],
            "image_type": "normal"
        })

    return training_data


def save_jsonl(data, filename):
    Path(filename).parent.mkdir(parents=True, exist_ok=True)
    with open(filename, "w", encoding="utf-8") as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    print(f"[ì €ì¥ ì™„ë£Œ] {filename} ({len(data)}ê°œ)")


def get_data(
    num_exception_samples: int = 10,
    num_normal_samples: int = 10,
    output_exception: str = "finetuning/samples/qa_03_03_exception.jsonl",
    output_normal: str = "finetuning/samples/qa_03_03_normal.jsonl",
    output_combined: str = "finetuning/samples/qa_03_03_combined.jsonl"
):
    """ë©”ì¸ í•¨ìˆ˜: í…ìŠ¤íŠ¸ ì§ˆì˜-ì‘ë‹µ ìƒì„± (ì´ë¯¸ì§€ëŠ” build_dataset.pyì—ì„œ ë§¤ì¹­)"""

    print("\n" + "="*60)
    print("qa_03_03: ì´ë¯¸ì§€ ìƒì„± ì˜ˆì™¸ì²˜ë¦¬ ë°ì´í„°ì…‹ ìƒì„±")
    print("="*60)

    print(f"\n### 1. ì´ë¯¸ì§€ ì˜ˆì™¸ ì¼€ì´ìŠ¤ ì§ˆì˜ ìƒì„± (GPT) - {num_exception_samples}ê°œ")
    exception_samples = generate_exception_queries(num_exception_samples)
    print(f"[ìƒì„± ì™„ë£Œ] ì´ë¯¸ì§€ ì˜ˆì™¸ ì¼€ì´ìŠ¤ {len(exception_samples)}ê°œ")

    print(f"\n### 2. ìŠ¤íƒ€ì¼/ì»¬ëŸ¬ ì˜ˆì™¸ ì¼€ì´ìŠ¤ ì§ˆì˜ ìƒì„± (GPT) - {num_normal_samples}ê°œ")
    normal_samples = generate_normal_queries_with_length(num_normal_samples)
    print(f"[ìƒì„± ì™„ë£Œ] ìŠ¤íƒ€ì¼/ì»¬ëŸ¬ ì˜ˆì™¸ ì¼€ì´ìŠ¤ {len(normal_samples)}ê°œ")

    print("\n### 3. í•™ìŠµ í¬ë§· ë³€í™˜")
    exception_data = convert_exception_to_training_format(exception_samples)
    normal_data = convert_normal_to_training_format(normal_samples)

    print("\n### 4. JSONL ì €ì¥")
    save_jsonl(exception_data, output_exception)
    save_jsonl(normal_data, output_normal)

    combined_data = exception_data + normal_data
    random.shuffle(combined_data)
    save_jsonl(combined_data, output_combined)

    print("\n### 5. ìƒì„± í†µê³„")
    print(f"  - ì´ë¯¸ì§€ ì˜ˆì™¸ ì¼€ì´ìŠ¤ (no_face, multi_face): {len(exception_data)}ê°œ")
    print(f"  - ìŠ¤íƒ€ì¼/ì»¬ëŸ¬ ì˜ˆì™¸ ì¼€ì´ìŠ¤ (unsupported_style, missing_style): {len(normal_data)}ê°œ")
    print(f"  - ì „ì²´ í†µí•©: {len(combined_data)}ê°œ")

    exception_type_counts = {}
    for s in exception_samples:
        t = s.get("type", "unknown")
        exception_type_counts[t] = exception_type_counts.get(t, 0) + 1

    print("\n  [ì´ë¯¸ì§€ ì˜ˆì™¸ ì¼€ì´ìŠ¤ ìœ í˜•ë³„]")
    for t, c in sorted(exception_type_counts.items()):
        print(f"    - {t}: {c}ê°œ")

    type_counts = {}
    for s in normal_samples:
        t = s.get("type", "unknown")
        type_counts[t] = type_counts.get(t, 0) + 1

    print("\n  [ìŠ¤íƒ€ì¼/ì»¬ëŸ¬ ì˜ˆì™¸ ì¼€ì´ìŠ¤ ìœ í˜•ë³„]")
    for t, c in sorted(type_counts.items()):
        print(f"    - {t}: {c}ê°œ")

    return {
        "exception": exception_data,
        "normal": normal_data,
        "combined": combined_data
    }


if __name__ == "__main__":
    data = get_data(
        num_exception_samples=100,
        num_normal_samples=100,
        output_exception="finetuning/samples/qa_03_03_exception.jsonl",
        output_normal="finetuning/samples/qa_03_03_normal.jsonl",
        output_combined="finetuning/samples/qa_03_03.jsonl"
    )